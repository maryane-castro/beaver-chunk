# ğŸ¦« Beaver-Chunk

<p align="center">
  <img src="assets/icon.png" 
       alt="Beaver-Chunk Logo"
       width="200"
       style="border-radius:20px;"/>
</p>

<h1 align="center">Sistema Modular de Processamento de Documentos<br>
    <b>Beaver-Chunk</b>
</h1>

## ğŸ“‹ VisÃ£o Geral

Beaver-Chunk Ã© uma biblioteca Python modular para construÃ§Ã£o de pipelines de processamento de documentos, especialmente projetada para aplicaÃ§Ãµes de RAG (Retrieval-Augmented Generation) e busca semÃ¢ntica. Com uma arquitetura baseada no padrÃ£o Factory, permite combinar diferentes componentes como blocos de LEGO para criar soluÃ§Ãµes personalizadas.

## âœ¨ CaracterÃ­sticas Principais

- ğŸ“„ **Document Loaders FlexÃ­veis**: Suporte para mÃºltiplos formatos (PDF, TXT)
- âœ‚ï¸ **EstratÃ©gias de Chunking Inteligentes**: Character-based e Recursive splitting
- ğŸ§® **Embeddings de Alta Qualidade**: IntegraÃ§Ã£o com modelos HuggingFace
- ğŸ’¾ **Armazenamento Vetorial Eficiente**: PersistÃªncia com ChromaDB
- ğŸ­ **PadrÃ£o Factory**: Interface unificada e extensÃ­vel
- ğŸ”§ **Arquitetura Modular**: FÃ¡cil adiÃ§Ã£o de novos componentes

## ğŸš€ InstalaÃ§Ã£o

### Usando Poetry (Recomendado)

```bash
poetry install
```

### Usando pip

```bash
pip install langchain-text-splitters langchain-community pypdf langchain-huggingface sentence-transformers langchain-chroma rank-bm25 torch
```

### Requisitos

- Python 3.11+
- PyTorch 2.0+
- DependÃªncias do LangChain

## ğŸ“– Guia de Uso

### 1ï¸âƒ£ Pipeline de IndexaÃ§Ã£o Completo

```python
from src.factory import (
    get_document_loader, 
    get_chunker, 
    get_embedding, 
    get_vector_store
)

# Etapa 1: Carregar documentos
loader = get_document_loader(
    name="pdf",  # OpÃ§Ãµes: "txt", "pdf"
    path_document="data/seu_documento.pdf"
)
documentos = loader.get_documents()

# Etapa 2: Dividir em chunks
chunker = get_chunker(
    name="recursive",  # OpÃ§Ãµes: "character", "recursive"
    chunk_size=500,
    chunk_overlap=50
)
chunks = chunker.split_document(documentos)

# Etapa 3: Gerar embeddings
embedding = get_embedding(
    name="huggingface",
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
embedding_model = embedding.get_instance_for_vector_store()

# Etapa 4: Armazenar vetores
vector_store = get_vector_store(
    name="chroma",
    collection_name="minha_colecao",
    embeddings=embedding_model,
    persist_directory="./chroma_db"
)
vector_store.add(chunks)
```

### 2ï¸âƒ£ Sistema de RecuperaÃ§Ã£o

```python
from src.factory import get_embedding, get_vector_store

# Configurar componentes
embedding = get_embedding(
    name="huggingface",
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
embedding_model = embedding.get_instance_for_vector_store()

# Conectar ao vector store existente
vector_store = get_vector_store(
    name="chroma",
    collection_name="minha_colecao",
    embeddings=embedding_model,
    persist_directory="./chroma_db"
)

# Criar retriever e buscar
retriever = vector_store.get_instance_vector_store().as_retriever(
    search_kwargs={"k": 5}  # Retornar top 5 resultados
)
resultados = retriever.invoke("Sua pergunta aqui")

# Processar resultados
for doc in resultados:
    print(f"ConteÃºdo: {doc.page_content}")
    print(f"Metadados: {doc.metadata}")
    print("-" * 50)
```

## ğŸ—ï¸ Arquitetura do Projeto

```
beaver-chunk/
â”œâ”€â”€ ğŸ“ src/
â”‚   â”œâ”€â”€ ğŸ“‚ document_loaders/        # Carregadores de documentos
â”‚   â”‚   â”œâ”€â”€ abstract_document_loaders.py
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”‚   â””â”€â”€ txt_loader.py
â”‚   â”œâ”€â”€ ğŸ“‚ chunking/                # EstratÃ©gias de divisÃ£o
â”‚   â”‚   â”œâ”€â”€ abstract_chunking.py
â”‚   â”‚   â”œâ”€â”€ character.py           # DivisÃ£o por caracteres
â”‚   â”‚   â””â”€â”€ recursive.py           # DivisÃ£o recursiva
â”‚   â”œâ”€â”€ ğŸ“‚ embedding/               # Modelos de embedding
â”‚   â”‚   â”œâ”€â”€ abstract_embedding.py
â”‚   â”‚   â””â”€â”€ huggingface.py
â”‚   â”œâ”€â”€ ğŸ“‚ vector_store/            # Armazenamento vetorial
â”‚   â”‚   â”œâ”€â”€ abstract_vector_store.py
â”‚   â”‚   â””â”€â”€ chroma.py
â”‚   â”œâ”€â”€ ğŸ“‚ utils/                   # Utilidades
â”‚   â”‚   â””â”€â”€ pos_processor/
â”‚   â””â”€â”€ ğŸ­ factory.py               # Factory pattern
â”œâ”€â”€ ğŸ“ exemple/                     # Exemplos prÃ¡ticos
â”‚   â”œâ”€â”€ how_use_document_loaders.py
â”‚   â”œâ”€â”€ how_use_chunking.py
â”‚   â”œâ”€â”€ how_use_embeddings.py
â”‚   â””â”€â”€ how_use_vector_store.py
â”œâ”€â”€ ğŸ“ data/                        # Dados de exemplo
â”œâ”€â”€ ğŸ“„ your_lego_indexing.py        # Script de indexaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ your_lego_retriever.py       # Script de recuperaÃ§Ã£o
â””â”€â”€ ğŸ“„ pyproject.toml               # ConfiguraÃ§Ã£o do projeto
```

## ğŸ”Œ Componentes DisponÃ­veis

### Document Loaders

| Loader | DescriÃ§Ã£o | ParÃ¢metros |
|--------|-----------|------------|
| `txt` | Carrega arquivos de texto simples | `path_document` |
| `pdf` | Extrai texto de PDFs | `path_document` |

### Chunking Strategies

| Strategy | DescriÃ§Ã£o | ParÃ¢metros |
|----------|-----------|------------|
| `character` | DivisÃ£o por nÃºmero de caracteres | `chunk_size`, `chunk_overlap` |
| `recursive` | DivisÃ£o recursiva preservando estrutura | `chunk_size`, `chunk_overlap` |

### Embeddings

| Model | DescriÃ§Ã£o | ParÃ¢metros |
|-------|-----------|------------|
| `huggingface` | Modelos do HuggingFace Hub | `model_name` |

### Vector Stores

| Store | DescriÃ§Ã£o | ParÃ¢metros |
|-------|-----------|------------|
| `chroma` | ChromaDB persistente | `collection_name`, `embeddings`, `persist_directory` |

## ğŸ¯ Exemplos PrÃ¡ticos

### Exemplo 1: Pipeline BÃ¡sico TXT â†’ Chroma

```python
# Script completo em your_lego_indexing.py
from src.factory import (
    get_document_loader,
    get_chunker,
    get_embedding,
    get_vector_store
)

# Pipeline simples
loader = get_document_loader("txt", path_document="data/mock/description.txt")
docs = loader.get_documents()

chunker = get_chunker("character", chunk_size=500, chunk_overlap=0)
chunks = chunker.split_document(docs)

embedding = get_embedding("huggingface", 
                         model_name="sentence-transformers/all-MiniLM-L6-v2")
embed_model = embedding.get_instance_for_vector_store()

store = get_vector_store("chroma", 
                        collection_name="my_collection",
                        embeddings=embed_model,
                        persist_directory="./chroma_langchain_db")
store.add(chunks)
```

### Exemplo 2: Busca SemÃ¢ntica

```python
# Script completo em your_lego_retriever.py
retriever = vector_store.get_instance_vector_store().as_retriever()
docs = retriever.invoke("what did the president say about ketanji brown jackson?")
```

## ğŸ”§ Extensibilidade

### Adicionando um Novo Document Loader

```python
# src/document_loaders/csv_loader.py
from src.document_loaders.abstract_document_loaders import AbstractDocumentLoader
import pandas as pd

class CSVLoader(AbstractDocumentLoader):
    def __init__(self, path_document, **kwargs):
        self.path = path_document
        self.kwargs = kwargs
    
    def get_documents(self):
        df = pd.read_csv(self.path, **self.kwargs)
        # Processar DataFrame para documentos
        return documents

# Registrar em src/factory.py
_LOADER_DOCUMENTS = {
    "csv": CSVLoader,
    # ... outros loaders
}
```

### Adicionando um Novo Modelo de Embedding

```python
# src/embedding/openai.py
from src.embedding.abstract_embedding import AbstractEmbedding

class OpenAIEmbedding(AbstractEmbedding):
    def __init__(self, api_key, model_name="text-embedding-ada-002"):
        # ImplementaÃ§Ã£o
        pass
    
    def get_instance_for_vector_store(self):
        # Retornar instÃ¢ncia compatÃ­vel com LangChain
        pass

# Registrar em src/factory.py
```

## ğŸ“Š Performance e OtimizaÃ§Ã£o

### RecomendaÃ§Ãµes de Chunk Size

- **Documentos tÃ©cnicos**: 500-1000 caracteres
- **Narrativas**: 1000-2000 caracteres
- **Q&A**: 200-500 caracteres

### Modelos de Embedding Recomendados

- **Uso geral**: `sentence-transformers/all-MiniLM-L6-v2`
- **MultilÃ­ngue**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Alta precisÃ£o**: `sentence-transformers/all-mpnet-base-v2`

## ğŸ› Troubleshooting

### Problema: MemÃ³ria insuficiente ao gerar embeddings

```python
# Processar em batches
chunks_batch = chunks[:100]  # Processar 100 por vez
vector_store.add(chunks_batch)
```

### Problema: Resultados de busca irrelevantes

```python
# Ajustar parÃ¢metros do retriever
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.5, "k": 10}
)
```

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ‘¥ Autores

- **Maryane Castro** - *Desenvolvimento inicial* - [maryane.castro993@gmail.com](mailto:maryane.castro993@gmail.com)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o muito bem-vindas! Por favor, siga estes passos:

1. Fork o projeto
2. Crie sua feature branch (`git checkout -b feature/NovaFuncionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/NovaFuncionalidade`)
5. Abra um Pull Request

### Guidelines para ContribuiÃ§Ã£o

- Mantenha o padrÃ£o de cÃ³digo existente
- Adicione testes para novas funcionalidades
- Atualize a documentaÃ§Ã£o conforme necessÃ¡rio
- Certifique-se de que todos os testes passem

## ğŸ“š Recursos Adicionais

- [DocumentaÃ§Ã£o LangChain](https://python.langchain.com/)
- [HuggingFace Hub](https://huggingface.co/models)
- [ChromaDB Docs](https://docs.trychroma.com/)
- [Sentence Transformers](https://www.sbert.net/)

## ğŸš¦ Status do Projeto

ğŸŸ¢ **Ativo** - Em desenvolvimento contÃ­nuo


---
